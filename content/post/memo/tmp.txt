



# 流れ

- 最尤推定とベイズ推定
  - 点推定か確率推定か？ $\theta$ か、$p(\theta)$ か？
  - 最尤推定とは likelihood を "最大化" することでパラメータを求める（likelihoodを使うことがイコール最尤法ではない）


- 変分推論
  - ELBO
- 平均場近似
- EMアルゴリズム
- 変分EMアルゴリズム
  - 何が変分なのか？

- 最尤推定的な取り扱い
- MAP
## 混合ガウス分布への適用






### 因子分解




# メモ

- EM アルゴリズムの欠点
  - $\theta$ が点推定であること（最尤法なので）
  - 隠れ変数が1層だけの場合にしか用いれない
- モデルエビデンス
  - モデルエビデンスの推論方法の一つ
- Reparametrization trick
  - サンプリングで誤差逆伝搬できない問題に対処する方法
  - 正規乱数の変換
- 点推定の欠点（=最尤法の欠点）
  - 単峰ではない分布などを表現できず、過学習になる --> ベイズ推定は確率的に予測し、誤差を表示できる
- q(Z)とは？
  - $p(Z|X)$ が求まらないので、$q(Z)$ を準備してそれで評価した代わりにしようというもの
- 変分ベイズ
  - 決定論的である



# 資料

- http://chasen.org/~daiti-m/paper/vb-to-vae.pdf
  - VAE は変分ベイズの正統進化のモデル？
  - 通常のNNと異なり、データを簡単に生成できる
    - 通常の autoencoder では latent vector に分布を定義していないので、新しくデータを生成できない
- https://www.ism.ac.jp/~daichi/paper/vb-nlp-tutorial.pdf
- https://proceedings-of-deim.github.io/DEIM2020/papers/F8-3.pdf


# Variational Autoencoder


$\beta$ VAE、FactorVAE、$\beta$-TCVAE
DIPVAE
JointVAE
CascadeVAE

$$
\argmax_\theta \ln p(X, Z|\theta)
$$


EM アルゴリズムは最尤推定の計算方法の一つで、潜在変数を持つモデルの likelihood を最大化するパラメータ $\theta$ を求めることを目的としています^[混合ガウス分布はEMアルゴリズムを使用するために、確率的に解釈して $p(z)$ を導入していました。もともと潜在変数を持っていなくても、EMアルゴリズムの枠組みに合わせてモデルを修正することで恩恵を受けることができます。]。

観測した変数と潜在変数のどちらの情報も分かっていれば（$X, Z$ は complete dataset と呼ばれます）^[こうなると最早 $Z$ が "潜在" 変数ではないのですが...。]、

の joint distribution $p(X, Z|\theta)$ の対数尤度からパラメータ $\theta$ を決定するのが straightforward な手法です。ただし実際には $Z$ がどのような値を取って $X$ と関係しているか分からないです（$X$ だけだと incomlete data と呼ばれます）。

$$
\ln p(X, Z|\theta) = \ln \sum_Z p(X, Z|\theta) = \ln p(X|\theta)
$$

ここで $p(X|\theta)$ の最適化は難しいが^[混合ガウス分布における $\ln p(\rm{X}|\pi, \mu, \Sigma) = \sum_{n=1}^N \ln \left\{\sum_{k=1}^K \pi_k N(\rm{x}_n|\mu_k, \Sigma_k) \right\}$]、$p(X,Z|\theta)$ の最適化は比較的簡単である場合を想定します。潜在変数の分布 $q(\rm{Z})$ を導入する^[「ベイズ推定 = 事前分布を導入する」と思っていたりすると「EMアルゴリズムは最尤推定の計算方法なのに、事前分布である $q(\rm{Z})$ を導入する...？」と混乱してしまいますが、最尤推定とはパラメータ $\theta$ の決定的な値を求める手法であるということを改めて確認しておきましょう。]ことで、対数尤度を次のように変形することができます：