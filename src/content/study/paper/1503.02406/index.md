---
title: "Deep Learning and the Information Bottleneck Principle"
pubDate: 2024-02-27T11:01:52+09:00
categories: ["arxiv"]
tags: []
description: ""
---
## はじめに

[YOLOv9](https://arxiv.org/pdf/2402.13616.pdf) の "§3.1 Information Bottleneck Principle" でも議論されている内容であり、自分にとっては初出であったためまとめてみました。


## Abstract

- information bottleneck として知られている先行研究
- 実験ではなく理論的展開がメインの論文で、これが叩き台となって様々な議論が行われている


## Information Bottlneck

情報ボトルネック（Information Bottleneck）とは情報理論の分野から生まれた考え方で、ある情報 $X$ から情報をなるべく落とさず圧縮した $T$ を作成することを目的とした手法です。

### 相互情報量（mutual information）

確率変数 $X, Y$ がどれくらい依存しているかを表現するための尺度として、相互情報量（mutual information）が次の式で定義されます。

$$
I(X, Y) = \int \int p(x,y) \frac{p(x,y)}{p(x)p(y)} dxdy \geq 0
$$

"依存" していない、つまり互いに独立な確率変数であれば、$X$ を観測しても $Y$ に関する情報は知ることができないため、相互情報量としてはゼロになります。実際、確率変数が独立であれば $p(x, y)=p(x)p(y)$ となるため定義式からゼロとなることが分かります。また、互いに "依存" しているときに相互情報量は最大値を取ります。以下にまとめておきます：

- $X, Y$ が独立である場合 $\to$ $I(X, Y)$ が最小 = 0
- $X, Y$ が依存し合っている場合 $\to$ $I(X, Y)$ が最大







## DNN に関する Information Bottleneck

ここで下図のようなシンプルな DNN を考えてみます。学習時には説明変数と目的変数 $(X, Y)$ との組み合わせが利用でき（左端）、隠れ層を通していくことで入力の $X$ を加工していき、最終出力の $\hat{Y}$ を計算する（右端）概念図です。

{{< figure src="20240227-184454.png" width="500" >}} 

前段の入力を元に逐次的に計算していくことから、一種のマルコフ連鎖過程として考えることができます。そのため、Data Processing Inequality（DPI; データ処理不等式）より以下の関係式が成り立ちます。

$$
I(Y; X) \geq I (Y; h_j) \geq I (Y; h_i) \geq I (Y; \hat{Y})
$$

ここで $h_{i}$ は $i$ 番目の隠れ層で、$i \geq j$（$i$ の方がより深い位置にある）であるとしています。層を経るごとに説明変数 $Y$ と特徴量ベクトルとの "依存関係" が薄れていくと解釈できます。

### 補足

YOLOv9 の説明が分かりやすかったので引用しておきます。

DNN では層が深くなるほど元々の入力データの情報が失われる傾向にあるため、学習時にはこの不完全なデータ（出力）を用いて重みの更新が行われることになります。この問題に対処する一つの方法は、モデル自体のサイズを大きくすることで保持できる情報量を物理的に増やすというものです。これは深さ（depth）よりも幅（width）を大きくすることでモデルを大きくすべきであるという一種の指針が得られることになり、近年の大規模モデルの発展とも合致します。


###  The Information Bottleneck Principle

DNN では中間層 $\hat{X}$ に関する学習を進めていくわけですが、いかに最適な中間層を設定するかが鍵となっています。情報理論の言葉を借りると、optimal representation である $\hat{X}$ を計算することになるのですが、これは次式で表される関係式から求めることができます。

$$
L = I(X, \hat{X}) - \beta I(\hat{X}, Y)
$$

この $L$ を最小化するような $\hat{X}$ を求めることが DNN の学習の目的です。
ただし実際にどのように求めるのかについてはこの論文の範疇を超えています。これらを応用したものとして例えば Entropy Loss などがあり、実装については別の研究があるようです。


$I(X, \hat{X})$ は最小化したい量で、$\hat{X}$ が最小十分統計量であることを目指します。また $I(\hat{X}, Y)$ は最大化したい量で、隠れ層の情報と目的変数との "依存関係" があることが好ましいことを意味しています。


## Reference

- https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/F1-2.pdf


