---
title: "価値反復に基づくアルゴリズム①"
pubDate: 2025/11/25
categories: ["機械学習"]
tags: ["強化学習", "ベルマン方程式"]
description: "強化学習における基本知識、価値関数について成り立つ再帰的なベルマン方程式についてをまとめています。"
---

import Figure from "@/components/blog/Figure.astro";

# 準備

## 状態集合、行動空間

状態集合をすべての状態からなる集合として定義し

$$
S = \{s_0, s_1, s_2, ..., s_N\}
$$

と表します。時間ステップごとに

$$
S_0, S_1, ..., S_t, ...
$$

とし、それぞれは $(s_0,s_1,...)$ の中どれかを取ることになります。行動も同様に $A$ として集合と、実際に取る値 $(a_0, a_1,...)$ を定義しておきます。

## 報酬

状態の遷移に対する報酬は報酬関数として

$$
R_{t+1} = f_r(S_t, A_t, S_{t+1})
$$

で定まります。現在の状態 $S_t$ から行動 $A_t$ を選択して 1 ステップ後 $S_{t+1}$ に遷移したときに、どういった報酬が与えられるかということを意味しています。
どういった行動を取るか、どういった状態に遷移するか、などに依存するため、「最適な行動を取って最適な状態へ移動する」ということを報酬という数値で表現することができます。

## 収益

強化学習では得られる報酬を用いて方策を決めていきますが、報酬は行動の即時的な良さを示す指標であるため、長期的に見た場合の行動の良し悪しを評価する必要があります。
そこで収益と呼ば得れる考えを導入します。

収益は、ある期間で得られた報酬の累積和のことで、現時刻 $t$ より将来の方向で和を取り

$$
G_t = \sum_{i=0}^T R_{t+1+i}
$$

で定義します。

ここまでの内容を図に表すと次のようになります。各時刻間の遷移は、マルコフ決定過程（Markov Decision Process: MDP）で定義されていて、どの時刻でどの状態を取るか、どの状態からどの状態へ遷移するかは確率的に記述されます。

<Figure
  src="20251124-083715.png"
  width={400}
  alt="図の説明"
  caption="強化学習の基本用語のまとめ"
/>

この図を見るとある状態からある状態へ遷移するときの行動 $a$ と状態とが 1 対 1 で対応しているように錯覚してしまいますが、実際には行動 $a$ を選択した際にどのような結果になるかは確率的です。そのため、行動 $a$ を選択したときに状態 $s$ から 状態 $s^\prime$へ移動する確率は

$$
P(s^\prime|s,a)
$$

として遷移確率として表現されます。

## 状態価値

収益は、計算する区間の初期状態と将来の行動に依存する確率的に変動する値です。そこで状態を条件として収益の期待値を計算し、これを方策を定義する際の指標とします。それが**状態価値関数**（価値関数）です。
状態価値関数（価値関数）は、

$$
V^\pi(s) = E^\pi[G_t|S_t=s]
$$

で定義されます。
初期状態 $S_t =s$ のもとで、ある方策 $\pi$ のもとで行動したときに得られる報酬の期待値が $V^\pi(s)$ です。これと同様に、
初期状態 $S_t =s$ のもとで、ある方策 $\pi^\prime$ のもとで行動したときに得られる報酬の期待値は $V^{\pi^\prime(s)}$ です。
つまり、ある状態 $s$ を固定したうえで方策の良し悪しを図る指標として扱うことができ

$$
V^{\pi^\prime(s)} \lessgtr V^{\pi(s)}
$$

の大小関係で、どちらの方策がより良いかを評価できます。

## 行動価値

状態価値に加えて、行動も条件に加えた

$$
Q^\pi(s,a) = E^\pi[G_t|S_t=s, A_t =a]
$$

も考えることができ、同様に行動価値関数（行動価値）を定義することができます。

# ベルマン方程式

強化学習の目的は最適な方策 $\pi$ を探索することで、状態価値関数もしくは行動価値関数を用いることで現在の方策 $\pi$ を評価することができるようになります。価値関数の値が大きくなるような方策を探索すればいいわけですが、例えば割引報酬和に関する価値関数を考えると

$$
\begin{align*}
V^\pi(s) &= E^\pi[G_t | S_t =s] \\
 &= E^\pi \left[\sum_{i=0}^\infty \gamma^i R_{t+1+i} | S_t =s \right]
\end{align*}
$$

となり、この期待値を計算する必要が生じます。この式を正確に計算するには無限時間先までの計算（$\sum_{i=0}^\infty$ の計算）が必要になるため、そのままの形で計算することができません。

そこで価値関数を評価しやすい逐次的な処理に落とし込んだ関係式が、ベルマン方程式です。価値観数の$t+1$ の項と、$t+2$ 以降の項とで計算を分離し、
まとめると次の関係式（ベルマン方程式）が導出されます。

$$
V^\pi(s)= \sum_{a\in A} \pi(a|s)\sum_{s^\prime \in S} P(s^\prime | s, a) (f_r(s,a,s^\prime) + \gamma V^\pi(s^\prime))
$$

## 導出

状態価値関数は期待値の線形性を利用することで

$$
\begin{align*}
V^\pi(s) &= E^\pi[G_t | S_t =s] \\
 &= E^\pi [R_{t+1}|S_t=s] + E^\pi [ \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t = s]
\end{align*}
$$

$t+1$ の項について、$S_t=s$ の条件のもとで得られる報酬の期待値を考えます。行動 $A_t=a$ を選択し 1 ステップ後に $S_{t+1}=s^\prime$となる確率は

$$
P(s^\prime|s,a) \pi(a|s)
$$

で表されるため、このときに得られる報酬は

$$
P(s^\prime|s,a) \pi(a|s) \times r(s,a,s^\prime)
$$

です。$a$ と $s^\prime$ について全てを足し合わせると$S_t=s$ の条件のもとで得られる報酬の期待値となるため、

$$
E^\pi [R_{t+1}|S_t=s] = \sum_{a \in A} \pi(a|s)\sum_{s^\prime \in S} P(s^\prime|s,a) r(s,a,s^\prime)
$$

と計算できます。$t+2$ 以降の項については、

$$
\begin{align*}
  &E^\pi [ \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t = s] \\
  &= \sum_{a\in A} \pi(a|s) \sum_{s^\prime in S} P(s^\prime|s,a) E^\pi [R_{t+2} + \gamma R_{t+3} + ... | S_{t+1}=s^\prime] \\
  &= \sum_{a\in A} \pi(a|s) \sum_{s^\prime in S} P(s^\prime|s,a) V^\pi(s^\prime)
\end{align*}
$$

となり、これらをまとめることで、ベルマン方程式が導出できます。

## 次回以降で

しかしベルマン方程式から直接価値関数を求めるには、状態遷移確率 $P(s^\prime|s,a)$ が分かっている必要がありこの前提も一般的ではありません。そこで状態遷移確率の導出なしでベルマン方程式（価値関数）の値を計算する手法が考案されています。

ベルマン方程式を解くための具体的なアルゴリズムについては次回以降で議論していきたいと思います。
